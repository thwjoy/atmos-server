{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install Whisper from GitHub repository\n",
    "\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import urllib\n",
    "import tarfile\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy.io import wavfile\n",
    "from tqdm.notebook import tqdm\n",
    "from dtw import dtw\n",
    "from whisper.tokenizer import get_tokenizer\n",
    "from scipy.ndimage import median_filter\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected language: English (en_us)\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "languages = {\"af_za\": \"Afrikaans\", \"am_et\": \"Amharic\", \"ar_eg\": \"Arabic\", \"as_in\": \"Assamese\", \"az_az\": \"Azerbaijani\", \"be_by\": \"Belarusian\", \"bg_bg\": \"Bulgarian\", \"bn_in\": \"Bengali\", \"bs_ba\": \"Bosnian\", \"ca_es\": \"Catalan\", \"cmn_hans_cn\": \"Chinese\", \"cs_cz\": \"Czech\", \"cy_gb\": \"Welsh\", \"da_dk\": \"Danish\", \"de_de\": \"German\", \"el_gr\": \"Greek\", \"en_us\": \"English\", \"es_419\": \"Spanish\", \"et_ee\": \"Estonian\", \"fa_ir\": \"Persian\", \"fi_fi\": \"Finnish\", \"fil_ph\": \"Tagalog\", \"fr_fr\": \"French\", \"gl_es\": \"Galician\", \"gu_in\": \"Gujarati\", \"ha_ng\": \"Hausa\", \"he_il\": \"Hebrew\", \"hi_in\": \"Hindi\", \"hr_hr\": \"Croatian\", \"hu_hu\": \"Hungarian\", \"hy_am\": \"Armenian\", \"id_id\": \"Indonesian\", \"is_is\": \"Icelandic\", \"it_it\": \"Italian\", \"ja_jp\": \"Japanese\", \"jv_id\": \"Javanese\", \"ka_ge\": \"Georgian\", \"kk_kz\": \"Kazakh\", \"km_kh\": \"Khmer\", \"kn_in\": \"Kannada\", \"ko_kr\": \"Korean\", \"lb_lu\": \"Luxembourgish\", \"ln_cd\": \"Lingala\", \"lo_la\": \"Lao\", \"lt_lt\": \"Lithuanian\", \"lv_lv\": \"Latvian\", \"mi_nz\": \"Maori\", \"mk_mk\": \"Macedonian\", \"ml_in\": \"Malayalam\", \"mn_mn\": \"Mongolian\", \"mr_in\": \"Marathi\", \"ms_my\": \"Malay\", \"mt_mt\": \"Maltese\", \"my_mm\": \"Myanmar\", \"nb_no\": \"Norwegian\", \"ne_np\": \"Nepali\", \"nl_nl\": \"Dutch\", \"oc_fr\": \"Occitan\", \"pa_in\": \"Punjabi\", \"pl_pl\": \"Polish\", \"ps_af\": \"Pashto\", \"pt_br\": \"Portuguese\", \"ro_ro\": \"Romanian\", \"ru_ru\": \"Russian\", \"sd_in\": \"Sindhi\", \"sk_sk\": \"Slovak\", \"sl_si\": \"Slovenian\", \"sn_zw\": \"Shona\", \"so_so\": \"Somali\", \"sr_rs\": \"Serbian\", \"sv_se\": \"Swedish\", \"sw_ke\": \"Swahili\", \"ta_in\": \"Tamil\", \"te_in\": \"Telugu\", \"tg_tj\": \"Tajik\", \"th_th\": \"Thai\", \"tr_tr\": \"Turkish\", \"uk_ua\": \"Ukrainian\", \"ur_pk\": \"Urdu\", \"uz_uz\": \"Uzbek\", \"vi_vn\": \"Vietnamese\", \"yo_ng\": \"Yoruba\"}\n",
    "# selection = widgets.Dropdown(\n",
    "#     options=[(\"Select language\", None), (\"----------\", None)] + sorted([(f\"{v} ({k})\", k) for k, v in languages.items()]),\n",
    "#     value=\"en_us\",\n",
    "#     description='Language:',\n",
    "#     disabled=False,\n",
    "# )\n",
    "\n",
    "lang = \"en_us\"\n",
    "language = \"English\"\n",
    "\n",
    "# assert lang is not None, \"Please select a language\"\n",
    "print(f\"Selected language: {language} ({lang})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, target_path: str):\n",
    "    with urllib.request.urlopen(url) as source, open(target_path, \"wb\") as output:\n",
    "        with tqdm(total=int(source.info().get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n",
    "            while True:\n",
    "                buffer = source.read(8192)\n",
    "                if not buffer:\n",
    "                    break\n",
    "\n",
    "                output.write(buffer)\n",
    "                loop.update(len(buffer))\n",
    "\n",
    "\n",
    "class Fleurs(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A simple class to wrap Fleurs and subsample a portion of the dataset as needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, lang, split=\"test\", subsample_rate=1, device=DEVICE):\n",
    "        url = f\"https://storage.googleapis.com/xtreme_translations/FLEURS102/{lang}.tar.gz\"\n",
    "        tar_path = os.path.expanduser(f\"~/.cache/fleurs/{lang}.tgz\")\n",
    "        os.makedirs(os.path.dirname(tar_path), exist_ok=True)\n",
    "\n",
    "        if not os.path.exists(tar_path):\n",
    "            download(url, tar_path)\n",
    "\n",
    "        all_audio = {}\n",
    "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "            for member in tar.getmembers():\n",
    "                name = member.name\n",
    "                if name.endswith(f\"{split}.tsv\"):\n",
    "                    labels = pd.read_table(tar.extractfile(member), names=(\"id\", \"file_name\", \"raw_transcription\", \"transcription\", \"_\", \"num_samples\", \"gender\"))\n",
    "\n",
    "                if f\"/{split}/\" in name and name.endswith(\".wav\"):\n",
    "                    audio_bytes = tar.extractfile(member).read()\n",
    "                    all_audio[os.path.basename(name)] = wavfile.read(io.BytesIO(audio_bytes))[1]                    \n",
    "\n",
    "        self.labels = labels.to_dict(\"records\")[::subsample_rate]\n",
    "        self.all_audio = all_audio\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        record = self.labels[item]\n",
    "        \n",
    "        audio = torch.from_numpy(self.all_audio[record[\"file_name\"]].copy())\n",
    "        text = record[\"transcription\"]\n",
    "        \n",
    "        return (audio, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Fleurs(lang, subsample_rate=100)  # subsample 10% of the dataset for a quick demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomjoy/Documents/GitHub/atmos-server/whisper/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is multilingual and has 71,825,920 parameters.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46818fdd8f5e43b8af6e55855646e1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomjoy/Documents/GitHub/atmos-server/whisper/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "from whisper import whisper\n",
    "whisper.model.MultiHeadAttention.use_sdpa = False\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "print(\n",
    "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
    "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
    ")\n",
    "\n",
    "options = dict(language=language, beam_size=5, best_of=5)\n",
    "transcribe_options = dict(task=\"transcribe\", **options)\n",
    "# translate_options = dict(task=\"translate\", **options)\n",
    "\n",
    "# references = []\n",
    "transcriptions = []\n",
    "# translations = []\n",
    "\n",
    "for audio, text in tqdm(dataset):\n",
    "    transcription = model.transcribe(audio, **transcribe_options)[\"text\"]\n",
    "    # translation = model.transcribe(audio, **translate_options)[\"text\"]\n",
    "    \n",
    "    transcriptions.append(transcription)\n",
    "    # translations.append(translation)\n",
    "    # references.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unfortunately, stunning traffic flow is difficult because driver behavior cannot be predicted with 100% certainty.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dr. Mala Bala Subramanian 29 was found in Blue Ash, Ohio, a suburb approximately 15 miles north of Cincinnati, lying on the ground beside the road in a t-shirt and underwear in an apparently heavily medicated state.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Goats seemed to have been first domesticated roughly 10,000 years ago in the Zagros Mountains of Iran.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 25 Dunlap Broadside's still known to exist are the oldest surviving copies of the document. The original handwritten copy has not survived.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elements like calcium and potassium are considered metals, of course there are also metals like silver and gold.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The pH levels level is indicated by the amount of hydrogen, the age and pH, ions in the tested chemical.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Aristotle, philosopher, theorized that everything is made up of a mixture of one or more four elements. They were earth, water, air, and fire.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                              transcription\n",
       "0                                                                                                        Unfortunately, stunning traffic flow is difficult because driver behavior cannot be predicted with 100% certainty.\n",
       "1   Dr. Mala Bala Subramanian 29 was found in Blue Ash, Ohio, a suburb approximately 15 miles north of Cincinnati, lying on the ground beside the road in a t-shirt and underwear in an apparently heavily medicated state.\n",
       "2                                                                                                                    Goats seemed to have been first domesticated roughly 10,000 years ago in the Zagros Mountains of Iran.\n",
       "3                                                                           The 25 Dunlap Broadside's still known to exist are the oldest surviving copies of the document. The original handwritten copy has not survived.\n",
       "4                                                                                                          Elements like calcium and potassium are considered metals, of course there are also metals like silver and gold.\n",
       "5                                                                                                                  The pH levels level is indicated by the amount of hydrogen, the age and pH, ions in the tested chemical.\n",
       "6                                                                            Aristotle, philosopher, theorized that everything is made up of a mixture of one or more four elements. They were earth, water, air, and fire."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(dict(transcription=transcriptions))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from whisper.tokenizer import get_tokenizer\n",
    "from dtw import dtw\n",
    "from scipy.ndimage import median_filter\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_SAMPLES_PER_TOKEN = whisper.audio.HOP_LENGTH * 2\n",
    "AUDIO_TIME_PER_TOKEN = AUDIO_SAMPLES_PER_TOKEN / whisper.audio.SAMPLE_RATE\n",
    "\n",
    "medfilt_width = 7\n",
    "qk_scale = 1.0\n",
    "\n",
    "tokenizer = get_tokenizer(model.is_multilingual, language=languages[lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tokens_on_unicode(tokens: torch.Tensor):\n",
    "    words = []\n",
    "    word_tokens = []\n",
    "    current_tokens = []\n",
    "    \n",
    "    for token in tokens.tolist():\n",
    "        current_tokens.append(token)\n",
    "        decoded = tokenizer.decode_with_timestamps(current_tokens)\n",
    "        if \"\\ufffd\" not in decoded:\n",
    "            words.append(decoded)\n",
    "            word_tokens.append(current_tokens)\n",
    "            current_tokens = []\n",
    "    \n",
    "    return words, word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tokens_on_spaces(tokens: torch.Tensor):\n",
    "    subwords, subword_tokens_list = split_tokens_on_unicode(tokens)\n",
    "    words = []\n",
    "    word_tokens = []\n",
    "    \n",
    "    for subword, subword_tokens in zip(subwords, subword_tokens_list):\n",
    "        special = subword_tokens[0] >= tokenizer.eot\n",
    "        with_space = subword.startswith(\" \")\n",
    "        punctuation = subword.strip() in string.punctuation\n",
    "        if special or with_space or punctuation:\n",
    "            words.append(subword)\n",
    "            word_tokens.append(subword_tokens)\n",
    "        else:\n",
    "            words[-1] = words[-1] + subword\n",
    "            word_tokens[-1].extend(subword_tokens)\n",
    "    \n",
    "    return words, word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install hooks on the cross attention layers to retrieve the attention weights\n",
    "QKs = [None] * model.dims.n_text_layer\n",
    "\n",
    "# Define the hook function explicitly\n",
    "def save_attention_weights(module, input, output, index):\n",
    "    QKs[index] = output[-1]  # Save the attention weights for each layer\n",
    "\n",
    "# Register the forward hooks explicitly\n",
    "for i, block in enumerate(model.decoder.blocks):\n",
    "    block.cross_attn.register_forward_hook(lambda module, input, output, i=i: save_attention_weights(module, input, output, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.9385e-05, 1.4842e-05,\n",
      "        7.7486e-06])\n"
     ]
    }
   ],
   "source": [
    "# for the first 10 examples in the dataset\n",
    "sampliing = 16000\n",
    "AUDIO_TIME_PER_TOKEN = AUDIO_SAMPLES_PER_TOKEN / whisper.audio.SAMPLE_RATE\n",
    "# audio_file = \"buffers/buffer_0.wav\"\n",
    "# audio_full = torch.tensor(whisper.load_audio(audio_file))\n",
    "# transcription = \"When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story starts.\"\n",
    "\n",
    "audio_full = dataset[0][0]\n",
    "print(audio_full.max(), audio_full.min())\n",
    "wavfile.write(\"notebook_wav.wav\", 16000, audio_full.numpy())\n",
    "transcription = transcriptions[0]\n",
    "\n",
    "def get_audio_index_at_time(time: float):\n",
    "    return int(time * whisper.audio.SAMPLE_RATE)\n",
    "\n",
    "# for (audio, label), transcription in zip(dataset, transcriptions):\n",
    "print(transcription)\n",
    "\n",
    "duration = len(audio_full)\n",
    "audio_len = len(audio_full) / whisper.audio.SAMPLE_RATE\n",
    "chunk_len_s = 2\n",
    "\n",
    "mel = whisper.log_mel_spectrogram(whisper.pad_or_trim(audio))\n",
    "tokens = torch.tensor(\n",
    "    [\n",
    "        *tokenizer.sot_sequence,\n",
    "        tokenizer.timestamp_begin,\n",
    "    ] + tokenizer.encode(transcription) + [\n",
    "        tokenizer.timestamp_begin + duration // AUDIO_SAMPLES_PER_TOKEN,\n",
    "        tokenizer.eot,\n",
    "    ]\n",
    ")\n",
    "with torch.no_grad():\n",
    "    logits = model(mel.unsqueeze(0), tokens.unsqueeze(0))\n",
    "    print(\"Raw transcript: \", tokenizer.decode(logits.squeeze(0).argmax(dim=-1)))\n",
    "\n",
    "print(mel.shape, tokens.shape)\n",
    "weights = torch.cat(QKs)  # layers * heads * tokens * frames    \n",
    "weights = weights[:, :, :, : duration // AUDIO_SAMPLES_PER_TOKEN].cpu()\n",
    "weights = median_filter(weights, (1, 1, 1, medfilt_width))\n",
    "weights = torch.tensor(weights * qk_scale).softmax(dim=-1)\n",
    "\n",
    "w = weights / weights.norm(dim=-2, keepdim=True)\n",
    "matrix = w[-6:].mean(axis=(0, 1))\n",
    "\n",
    "alignment = dtw(-matrix.double().numpy())\n",
    "\n",
    "jumps = np.pad(np.diff(alignment.index1s), (1, 0), constant_values=1).astype(bool)\n",
    "jump_times = alignment.index2s[jumps] * AUDIO_TIME_PER_TOKEN\n",
    "words, word_tokens = split_tokens_on_spaces(tokens) \n",
    "\n",
    "# display the word-level timestamps in a table\n",
    "word_boundaries = np.pad(np.cumsum([len(t) for t in word_tokens[:-1]]), (1, 0))\n",
    "begin_times = jump_times[word_boundaries[:-1]]\n",
    "end_times = jump_times[word_boundaries[1:]]\n",
    "\n",
    "# we only want to display words up to the current time\n",
    "\n",
    "\n",
    "data = [\n",
    "    dict(word=word, begin=begin, end=end)\n",
    "    for word, begin, end in zip(words, begin_times, end_times)\n",
    "    if not word.startswith(\"<|\") and word.strip() not in \".,!?、。\" \n",
    "]\n",
    "\n",
    "display(pd.DataFrame(data))\n",
    "display(HTML(\"<hr>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
